# =============================================================================
# FIXED MASTER SCRIPT: Trust-Oriented Evaluation (MIT-BIH)
# NOW INCLUDES: Baseline, Robust Proxy, TimeGAN, and LSTM-VAE
# =============================================================================

import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from dataclasses import dataclass
from typing import Dict, List
import warnings

# --- 1. SETUP & INSTALLATION ---
try:
    import wfdb
except ImportError:
    print("Installing wfdb...")
    os.system('pip install wfdb')
    import wfdb

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    log_loss,
    recall_score
)

# Suppress warnings for clean output
warnings.filterwarnings("ignore")


# =============================================================================
# 2. CONFIGURATION
# =============================================================================
@dataclass
class Config:
    # EXPERIMENT SETTINGS
    seeds: List[int] = (19, 88, 123)
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

    # DATASET
    data_dir: str = "data_mitbih"
    db_name: str = "mitdb"
    beat_len: int = 256  # Fixed beat length
    pre_r: int = 90  # Samples before R-peak
    post_r: int = 166  # Samples after R-peak
    channel: int = 0  # Lead MLII
    train_frac: float = 0.7
    per_class_cap: int = 200  # Max samples per class per record

    # GENERATOR SETTINGS
    baseline_noise: float = 0.35
    robust_noise: float = 0.08

    # TimeGAN Settings
    tg_hidden: int = 24
    tg_layers: int = 3
    tg_batch: int = 64
    tg_lr: float = 0.001
    tg_epochs: int = 150

    # LSTM-VAE Settings (New)
    vae_hidden: int = 64
    vae_latent: int = 20
    vae_layers: int = 1
    vae_batch: int = 64
    vae_lr: float = 0.001
    vae_epochs: int = 100 # VAE converges faster than GAN

    # EVALUATION METRICS
    latent_delta: float = 0.05
    diversity_sample: int = 500
    diversity_pairs: int = 2000
    rf_estimators: int = 50

    # Safety Constraints
    safety_min: float = -5.0
    safety_max: float = 5.0
    safety_slope: float = 4.0


cfg = Config()


def set_seeds(seed):
    """Sets random seeds for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


print(f"Running on {cfg.device} with seeds: {cfg.seeds}")

# =============================================================================
# 3. DATA LOADING
# =============================================================================
AAMI_MAP = {
    "N": "N", "L": "N", "R": "N", "e": "N", "j": "N",
    "A": "S", "a": "S", "J": "S", "S": "S",
    "V": "V", "E": "V",
    "F": "F",
    "/": "Q", "f": "Q", "Q": "Q", "?": "Q"
}
CLASSES = ["N", "S", "V", "F", "Q"]


def load_mitbih_data(seed):
    """Downloads and extracts MIT-BIH beats."""
    set_seeds(seed)

    # Download
    os.makedirs(cfg.data_dir, exist_ok=True)
    try:
        if not os.path.exists(os.path.join(cfg.data_dir, '100.hea')):
            print("Downloading MIT-BIH Database...")
            wfdb.dl_database(cfg.db_name, dl_dir=cfg.data_dir)
    except:
        pass

    # Get records
    recs = [f.split('.')[0] for f in os.listdir(cfg.data_dir) if f.endswith('.hea')]
    recs = [r for r in recs if r.isdigit()]
    random.shuffle(recs)

    # Split
    split_idx = int(len(recs) * cfg.train_frac)
    train_recs = recs[:split_idx]
    test_recs = recs[split_idx:]

    def extract(record_list):
        X, y = [], []
        for rec in record_list:
            try:
                path = os.path.join(cfg.data_dir, rec)
                ann = wfdb.rdann(path, 'atr')
                sig = wfdb.rdrecord(path).p_signal[:, cfg.channel]
            except:
                continue

            cnt = {c: 0 for c in CLASSES}
            for s, sym in zip(ann.sample, ann.symbol):
                if sym in AAMI_MAP:
                    label = AAMI_MAP[sym]
                    if cnt[label] < cfg.per_class_cap:
                        start, end = s - cfg.pre_r, s + cfg.post_r
                        if start >= 0 and end <= len(sig):
                            beat = sig[start:end]
                            # Z-score norm
                            beat = (beat - np.mean(beat)) / (np.std(beat) + 1e-6)
                            if len(beat) == cfg.beat_len:
                                X.append(beat)
                                y.append(CLASSES.index(label))
                                cnt[label] += 1
        return np.array(X, dtype=np.float32), np.array(y, dtype=int)

    print("  Extracting Train...")
    X_train, y_train = extract(train_recs)
    print("  Extracting Test...")
    X_test, y_test = extract(test_recs)

    return np.expand_dims(X_train, -1), y_train, np.expand_dims(X_test, -1), y_test


# =============================================================================
# 4A. FIXED TIMEGAN IMPLEMENTATION
# =============================================================================
class TimeGAN_Net(nn.Module):
    def __init__(self, d_in, d_out, hidden, layers, output_sig=False):
        super().__init__()
        self.rnn = nn.GRU(d_in, hidden, layers, batch_first=True)
        self.lin = nn.Linear(hidden, d_out)
        self.output_sig = output_sig
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        o, _ = self.rnn(x)
        o = self.lin(o)
        if self.output_sig:
            o = self.sigmoid(o)
        return o


class TimeGAN:
    def __init__(self):
        self.E = TimeGAN_Net(1, cfg.tg_hidden, cfg.tg_hidden, cfg.tg_layers).to(cfg.device)
        self.R = TimeGAN_Net(cfg.tg_hidden, 1, cfg.tg_hidden, cfg.tg_layers).to(cfg.device)
        self.G = TimeGAN_Net(1, cfg.tg_hidden, cfg.tg_hidden, cfg.tg_layers).to(cfg.device)
        self.S = TimeGAN_Net(cfg.tg_hidden, cfg.tg_hidden, cfg.tg_hidden, cfg.tg_layers).to(cfg.device)
        self.D = TimeGAN_Net(cfg.tg_hidden, 1, cfg.tg_hidden, cfg.tg_layers, output_sig=True).to(cfg.device)

        self.opt_e = optim.Adam(list(self.E.parameters()) + list(self.R.parameters()), lr=cfg.tg_lr)
        self.opt_g = optim.Adam(list(self.G.parameters()) + list(self.S.parameters()), lr=cfg.tg_lr)
        self.opt_d = optim.Adam(self.D.parameters(), lr=cfg.tg_lr)

        self.mse = nn.MSELoss()
        self.bce = nn.BCELoss()

    def train(self, X_train):
        dataset = TensorDataset(torch.FloatTensor(X_train))
        loader = DataLoader(dataset, batch_size=cfg.tg_batch, shuffle=True)

        ae_epoch = int(cfg.tg_epochs * 0.2)
        sup_epoch = int(cfg.tg_epochs * 0.2)
        joint_epoch = cfg.tg_epochs

        # 1. Embedding
        for _ in range(ae_epoch):
            for batch in loader:
                x = batch[0].to(cfg.device)
                l = self.mse(x, self.R(self.E(x)))
                self.opt_e.zero_grad(); l.backward(); self.opt_e.step()

        # 2. Supervisor
        for _ in range(sup_epoch):
            for batch in loader:
                x = batch[0].to(cfg.device)
                h = self.E(x).detach()
                l = self.mse(h[:, 1:], self.S(h)[:, :-1])
                self.opt_g.zero_grad(); l.backward(); self.opt_g.step()

        # 3. Joint
        for _ in range(joint_epoch):
            for batch in loader:
                x = batch[0].to(cfg.device)
                b_size = x.size(0)

                # Generator
                z = torch.randn(b_size, cfg.beat_len, 1).to(cfg.device)
                e_hat = self.G(z)
                h_hat = self.S(e_hat)
                x_hat = self.R(h_hat)
                y_fake = self.D(h_hat)
                h = self.E(x).detach()
                loss_g_adv = self.bce(y_fake, torch.ones_like(y_fake))
                loss_s = self.mse(h[:, 1:], self.S(h)[:, :-1])
                loss_mom = torch.mean(torch.abs(torch.mean(x_hat, 0) - torch.mean(x, 0))) + \
                           torch.mean(torch.abs(torch.std(x_hat, 0) - torch.std(x, 0)))
                loss_g = loss_g_adv + 10 * loss_s + 100 * loss_mom
                self.opt_g.zero_grad(); loss_g.backward(); self.opt_g.step()

                # Discriminator
                y_real = self.D(h)
                h_hat_d = self.S(self.G(z)).detach()
                y_fake_d = self.D(h_hat_d)
                loss_d = self.bce(y_real, torch.ones_like(y_real)) + \
                         self.bce(y_fake_d, torch.zeros_like(y_fake_d))
                self.opt_d.zero_grad(); loss_d.backward(); self.opt_d.step()

    def generate(self, n):
        self.G.eval(); self.S.eval(); self.R.eval()
        with torch.no_grad():
            z = torch.randn(n, cfg.beat_len, 1).to(cfg.device)
            return self.R(self.S(self.G(z))).cpu().numpy()

# =============================================================================
# 4B. LSTM-VAE IMPLEMENTATION (NEW)
# =============================================================================
class LSTM_VAE(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers=1):
        super(LSTM_VAE, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.latent_dim = latent_dim

        # Encoder
        self.encoder_lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)

        # Decoder
        self.decoder_input = nn.Linear(latent_dim, hidden_dim)
        self.decoder_lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)
        self.final_layer = nn.Linear(hidden_dim, input_dim)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        # Encode
        _, (h_n, _) = self.encoder_lstm(x)
        h_last = h_n[-1] # Take last layer
        mu = self.fc_mu(h_last)
        logvar = self.fc_logvar(h_last)
        z = self.reparameterize(mu, logvar)

        # Decode
        # Expand z to sequence length
        seq_len = x.size(1)
        # We repeat the latent vector to be the input for every timestep
        # Alternatively, we could use it as hidden state initialization.
        # Here: Project z back to hidden and repeat.
        d_in = self.decoder_input(z).unsqueeze(1).repeat(1, seq_len, 1)

        out, _ = self.decoder_lstm(d_in)
        recon_x = self.final_layer(out)

        return recon_x, mu, logvar

class VAE_Trainer:
    def __init__(self):
        self.model = LSTM_VAE(1, cfg.vae_hidden, cfg.vae_latent, cfg.vae_layers).to(cfg.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=cfg.vae_lr)

    def loss_function(self, recon_x, x, mu, logvar):
        MSE = nn.functional.mse_loss(recon_x, x, reduction='sum')
        # KL Divergence
        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
        return MSE + KLD

    def train(self, X_train):
        self.model.train()
        dataset = TensorDataset(torch.FloatTensor(X_train))
        loader = DataLoader(dataset, batch_size=cfg.vae_batch, shuffle=True)

        for _ in range(cfg.vae_epochs):
            for batch in loader:
                x = batch[0].to(cfg.device)
                recon_x, mu, logvar = self.model(x)
                loss = self.loss_function(recon_x, x, mu, logvar)
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

    def generate(self, n):
        self.model.eval()
        with torch.no_grad():
            z = torch.randn(n, cfg.vae_latent).to(cfg.device)
            # Decoder logic manually
            d_in = self.model.decoder_input(z).unsqueeze(1).repeat(1, cfg.beat_len, 1)
            out, _ = self.model.decoder_lstm(d_in)
            recon_x = self.model.final_layer(out)
            return recon_x.cpu().numpy()

# =============================================================================
# 5. METRIC FUNCTIONS
# =============================================================================
def compute_metrics(X_gen, y_gen, X_test, y_test):
    metrics = {}

    # A. Diversity
    idx = np.random.choice(len(X_gen), min(len(X_gen), cfg.diversity_sample), replace=False)
    sub = X_gen[idx].reshape(len(idx), -1)
    i1 = np.random.randint(0, len(sub), cfg.diversity_pairs)
    i2 = np.random.randint(0, len(sub), cfg.diversity_pairs)
    metrics['Diversity'] = np.mean(np.linalg.norm(sub[i1] - sub[i2], axis=1))

    # B. Safety
    valid = np.mean([1 if np.all(np.abs(x) <= 5) and np.all(np.abs(np.diff(x, axis=0)) <= 4) else 0 for x in X_gen])
    metrics['Safety'] = valid

    # C. Utility/Fairness/Fidelity
    clf = RandomForestClassifier(n_estimators=cfg.rf_estimators, n_jobs=-1)
    clf.fit(X_gen.reshape(len(X_gen), -1), y_gen)
    probs = clf.predict_proba(X_test.reshape(len(X_test), -1))
    preds = np.argmax(probs, axis=1)

    full_probs = np.zeros((len(y_test), 5))
    for i, c in enumerate(clf.classes_):
        if c < 5: full_probs[:, c] = probs[:, i]

    metrics['Fidelity'] = log_loss(y_test, full_probs, labels=[0, 1, 2, 3, 4])
    metrics['Accuracy'] = accuracy_score(y_test, preds)
    metrics['F1'] = f1_score(y_test, preds, average='macro')
    metrics['Fairness'] = 1.0 - np.std(recall_score(y_test, preds, average=None, labels=[0, 1, 2, 3, 4]))

    return metrics


def compute_robustness(X_seed, noise, model_type='proxy', model=None):
    if model_type == 'proxy':
        z = np.random.normal(0, 1, X_seed.shape)
        d = np.random.normal(0, cfg.latent_delta, X_seed.shape)
        return np.mean(((X_seed + noise * z) - (X_seed + noise * (z + d))) ** 2)

    elif model_type == 'timegan':
        z = torch.randn(200, cfg.beat_len, 1).to(cfg.device)
        d = torch.randn_like(z) * cfg.latent_delta
        model.G.eval(); model.S.eval(); model.R.eval()
        with torch.no_grad():
            x1 = model.R(model.S(model.G(z)))
            x2 = model.R(model.S(model.G(z + d)))
            return torch.mean((x1 - x2) ** 2).item()

    elif model_type == 'vae':
        z = torch.randn(200, cfg.vae_latent).to(cfg.device)
        d = torch.randn_like(z) * cfg.latent_delta
        model.model.eval()
        with torch.no_grad():
            # Decode z
            d_in1 = model.model.decoder_input(z).unsqueeze(1).repeat(1, cfg.beat_len, 1)
            out1, _ = model.model.decoder_lstm(d_in1)
            x1 = model.model.final_layer(out1)

            # Decode z + d
            d_in2 = model.model.decoder_input(z + d).unsqueeze(1).repeat(1, cfg.beat_len, 1)
            out2, _ = model.model.decoder_lstm(d_in2)
            x2 = model.model.final_layer(out2)

            return torch.mean((x1 - x2) ** 2).item()


# =============================================================================
# 6. MAIN LOOP
# =============================================================================
results_log = {"Baseline": [], "Robust": [], "TimeGAN": [], "LSTM-VAE": []}

print("=" * 60)
print("STARTING EXPERIMENT (Baseline, Robust, TimeGAN, LSTM-VAE)")
print("=" * 60)

for i, seed in enumerate(cfg.seeds):
    print(f"\n>>> SEED {seed} ({i + 1}/{len(cfg.seeds)})")

    # 1. Load Data
    X_tr, y_tr, X_te, y_te = load_mitbih_data(seed)

    # 2. Train TimeGAN
    print("  Training TimeGAN...")
    tg = TimeGAN()
    tg.train(X_tr)

    # 3. Train LSTM-VAE (New)
    print("  Training LSTM-VAE...")
    vae = VAE_Trainer()
    vae.train(X_tr)

    # Generate Synthetic Data
    X_tg_list, y_tg_list = [], []
    X_vae_list, y_vae_list = [], []

    for c in range(5):
        n = len(np.where(y_tr == c)[0])
        # TimeGAN
        X_tg_list.append(tg.generate(n))
        y_tg_list.append(np.full(n, c))
        # VAE
        X_vae_list.append(vae.generate(n))
        y_vae_list.append(np.full(n, c))

    X_tg = np.concatenate(X_tg_list)
    y_tg = np.concatenate(y_tg_list)
    X_vae = np.concatenate(X_vae_list)
    y_vae = np.concatenate(y_vae_list)

    # 4. Proxies
    def gen_p(n_val):
        xl, yl = [], []
        for c in range(5):
            idx = np.where(y_tr == c)[0]
            base = X_tr[np.random.choice(idx, len(idx), replace=True)]
            xl.append(base + n_val * np.random.normal(0, 1, base.shape))
            yl.append(np.full(len(idx), c))
        return np.concatenate(xl), np.concatenate(yl)

    X_b, y_b = gen_p(cfg.baseline_noise)
    X_r, y_r = gen_p(cfg.robust_noise)

    # 5. Evaluate
    print("  Evaluating...")
    m_b = compute_metrics(X_b, y_b, X_te, y_te)
    m_r = compute_metrics(X_r, y_r, X_te, y_te)
    m_t = compute_metrics(X_tg, y_tg, X_te, y_te)
    m_v = compute_metrics(X_vae, y_vae, X_te, y_te)

    m_b['Robustness'] = compute_robustness(X_tr, cfg.baseline_noise, 'proxy')
    m_r['Robustness'] = compute_robustness(X_tr, cfg.robust_noise, 'proxy')
    m_t['Robustness'] = compute_robustness(None, None, 'timegan', model=tg)
    m_v['Robustness'] = compute_robustness(None, None, 'vae', model=vae)

    results_log['Baseline'].append(m_b)
    results_log['Robust'].append(m_r)
    results_log['TimeGAN'].append(m_t)
    results_log['LSTM-VAE'].append(m_v)

# =============================================================================
# 7. FINAL TABLE
# =============================================================================
metrics_order = ["Fidelity", "Diversity", "Robustness", "Accuracy", "F1", "Fairness", "Safety"]
arrows = {
    "Fidelity": "↓",
    "Diversity": "↑",
    "Robustness": "↓",
    "Accuracy": "↑",
    "F1": "↑",
    "Fairness": "↑",
    "Safety": "↑"
}

print("\n" + "=" * 100)
print(r"\begin{table}[ht]")
print(r"\centering \small")
print(r"\caption{Results (Mean $\pm$ Std over 3 seeds)}")
print(r"\begin{tabular}{l c c c c}")
print(r"\toprule \textbf{Metric} & \textbf{Baseline} & \textbf{Robust} & \textbf{TimeGAN} & \textbf{LSTM-VAE} \\ \midrule")

for m in metrics_order:
    row = f"{m} ({arrows[m]})"
    for mod in ["Baseline", "Robust", "TimeGAN", "LSTM-VAE"]:
        v = [r[m] for r in results_log[mod]]

        if m == "Robustness":
            row += f" & {np.mean(v):.1e} $\\pm$ {np.std(v):.1e}"
        elif m == "Fidelity":
            row += f" & {np.mean(v):.3f} $\\pm$ {np.std(v):.3f}"
        else:
            row += f" & {np.mean(v):.4f} $\\pm$ {np.std(v):.4f}"

    print(row + r" \\")

print(r"\bottomrule \end{tabular} \end{table}")